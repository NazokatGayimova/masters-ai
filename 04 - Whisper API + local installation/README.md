Here steps that I followed to process audio to text using Whisper. 
1. I used web version of Whisper: https://ichigo-whisper.homebrew.ltd/
2. I uploaded the audio and I clicked "Transcripe with Whisper"
3. I have the text of whole audio:
   <img width="1387" alt="Screenshot 2025-01-06 at 15 47 02" src="https://github.com/user-attachments/assets/0db93b01-a70c-4357-a979-4841ab7837ca" />


 "Good evening, guys. Hello. Do you choose Mr. studies all or not yet? Is there anyone who has an eye to the Maybe two, three minutes ago. Thank you. Today the deadline. Yeah. It's just the by the way, by the way, do we have strict deadlines for homework, guys? No, isn't it? That's trick. Because I know it's not like I did. It was bad, but I would suggest to find at least like 10 to 15 minutes every day. How I am doing because when it will come at the end, it will be like everything, you know, so at least I'm just trying to find 10 to 15, 20 minutes doing doing like by small, small, small parts. Then I think they. Disco or from others. Any, I think. Other courses, I think, didn't give you homework yet. Yeah. They give it. They give it. Data data processing. They gave it gave. He gave yesterday. I think this file formats. I guess they I missed the classes they. Watch it. Hello, Italy. Hello, Italy. What are you doing? I have a question. About the future. If you see today, you say go and Google introduced quantum computers. Yeah, our second lesson, you said that there is a problem with the. Because they have a resource problem and what will be in the future about it? Because if Google if we have they have a quantum computers, then they have a huge result. This is the source. And how they will. Yeah, so that's that's really good. I don't know how they work like only very general concept of these cubits. I still, you know, I've been studying quantum physics in university like at least one year, probably two years. I can't remember. You know, a lot of. Electronics works on quantum effects. So, for example, some. Some stuff allows the electrons to jump the barrier. They like actually cannot jump. So they like tunneling through that. And no one knows like how it works, but it works. And we use it in TVs and smartphones. That's it. Like some some kind of some kind of magic. So it's still I think they should be some kind of scientific explanation. I don't know like how these quantum computers work. I don't understand the concept of like something having the value of zero and one at the same time, because like, OK, you can imagine that. But how about the storage? Right. So if I talk about like HDD or SSD, any storage, so you need to write it and you need to like you should be able to read it in a few minutes. And the most fear like two fears I remember like from the community regarding quantum computers is OK, if they are so like powerful and can make like much more like I don't know, millions times computations per second. So what about the encryption? Right. So I would say or any other encryption algorithm. So basically encryption works. Like modern encryption works on the very basic example like there are how it's called like one way function in maths. I don't know how it's in English. The strong you function. So the way like it's very easy to calculate this. For example, it's very easy to multiply it to prime. And get the result. But it's super hard to understand like what what are the integers you multiply it if you received like this result and a lot of encryption is based on that. And actually, Bitcoin is based on that. So Bitcoin has several LTS and it's based on that. So it's very easy to understand. Like technology sent most of them like top tier encryption we have right now. So if Google can beat that so they can possibly like hijack all the new bitcoins. Maybe they can rewrite the whole bitcoin. But it's not like that. So I would say if you want to like get a little bit more like a new encryption. Maybe they can rewrite the whole bitcoin like the whole blockchain like in a day and I don't know like steal all the bitcoins from us. So a lot of like military applications like all this encryption and networking. So I don't know. It's technically possible that such technology can appear. But taking into account how much time and money and resources and smart people it takes to build such thing. So maybe the first country which builds it, you know, like will create some kind of like snow crash like from cyberpunk books. I don't know. So we'll see. I know like from what I understand like IT history and technology history like all the new like top tier technologies. They have like two applications like the two industries basically driving them fast and like making them broadly available is porn industry and military. So this is like two things usually picking up the latest stage and trying to get like money of that. And maybe one of them will pick it up. I don't know. Maybe military first. So we're going to see the new applications of quantum stuff. Yeah. But from personal perspective, like being a human like I don't know like IT guy. So yeah, that's interesting. Unless I cannot benefit from that, I can ask unless I cannot like earn money doing that. So it's inevitable for me like the sun. The sun just can stop. I don't know. Like, OK, but example because the Earth rotates across the sun. So the Earth can just stop rotating and it will be infinite night for me. So I can't do anything about that. I can't do anything about rain. So what I can is just to buy the umbrella. I don't know, pray the rain stops sometimes. And in case I have the problem or some something I cannot influence, I think it's good not to worry about it. Because if you worry about something you cannot change, that's bad. So you spent a lot of resources and the result will be nothing like for sure. So my plan is just read the news. I don't know, follow this agenda, but don't worry about it. OK, yeah, let's get down to business. So let me try to open my schedule. I had to reboot my PC today, so I lost this Excel spreadsheet. OK, this one. OK, so the good news, good news, we are on track, right? And here we are, meeting number five. So today we're going to discuss Whisper API and local installation. The bad news, I took a look at some of the home works, like two or three, maybe five, but not like very, very high level. I like the blog posts you guys made. So I like the pictures and I think I will share some of my favorite pictures next time. So I assume I will need this weekend to check them and probably to, I don't know, set some marks or something like that. So just give me some time, but I haven't found any like critical problems there. So just remember to share the workflow, how you did that, like for the first task. So I need to see like what prompts you've been using and don't forget about the pictures. So the pictures are necessary for our blog post. So I think we're going to focus on that probably either on this lecture or this one. We're going to have time to review the home work. So today we're going to discuss the Whisper API and this is very... Let me show the slides. So here are the slides for today. So what is the Whisper and why are we discussing that? So mostly when people discuss the generative AI or AI in general, so they focused on like LLMs, I don't know, like compilers, functions and stuff like that. So everything that works with text, right? Because it has more application to the business than you work with text. But still, usually what is being forgotten is images and audio processing. And regarding the images, I can't say this like easier to monetize this. So not too much business applications. So I know a lot of cases where clients came to us like to EPM and asked to create something like, I don't know, maybe a read of over 100 of cases. And only several of them were related to image generation. But some of them, like more of these cases are related to audio. And one of the usual like requests is, okay, guys, we have 5,000 hours of recordings of, I don't know, meetings or customers calling us for the support. So we need to understand this data, right? So imagine you have a first line support, like people calling, like real users calling, describing the issue and the operators are just sharing. So what should you do to solve it? And you need to control it somehow, right? So you need to see what are the most popular questions. So what are the answers? So is the client is happy or not? Right. And for this case, you need to voice processing. And processing the voice is actually very old technology. I don't remember how old it is. But like two things here usually working is text to speech and speech to text. And if we check, I don't know, like let's check Azure text to speech. Okay, it's a yay speech now. All right, I got it. Everything is a yay now. Let me check the pricing, actually. DTS pricing. It's super cheap. It's super cheap. Okay, I think here. So this is, okay, free tier. All right. Basically, all right, delete this one. So speech to text. So in order to transcript something, like if you have a phone call, real time transcription, $1 per hour, that's too much. Okay, batch transcription. So if I have a lot of audio calls, I didn't know like one unread, I can post them as a batch. And this will result me in 80 cents per hour. So not too much, actually. Yeah, as usually, like Microsoft has like very complicated, very complicated stuff for billing. So it's hard to understand how much you will spend. Yeah, and here is like the text to speech. So basically two directions, like from speech, you can extract text and from text, you can generate speech, right? And this is like the standard voice. I think this standard voice become neural like during like last year because previously they had different, they had a different pricing. Like the standard voice is like this robotized voice you usually hear when you call a bank or something. And neural voice is like more pleasant, more realistically, more naturally told. So right now I don't think they should have some kind of like cheap voice like this old cheap voice. Probably it's inside the Azure AI Studio, but the trick is it's not expensive, right? So one million characters is like it's a lot. $15 for business, it's like, well, not too much, I think. And in case we have 1000 of like phone calls transcripted, so it will result as is only like less than $200. So that's acceptable for that. And they definitely use machine learning there, right? So because there's no way programmatically, like algorithmically to transcribe the speech. But still the interesting thing here is it's also possible to use machine learning techniques and all this concept of token we've been discussing previously, right? In working with speech, in understanding the speech and the project, the project I would like to show you this Whisper. So let me find it should be here in the notes. Yeah, this one. It was introduced very similar once the ChartGPT arrived, right? So I'm just trying to recognize what was the time when ChartGPT released? Okay, November 30th, alright, 2022. Yeah, so they introduced Whisper like a couple of months before ChartGPT. And you know what happens like, do you remember this movie, Sertin's Floor? This one. So do you know this movie? Sertin's Floor? It's kind of like popular from scientific perspective. So I don't know if you know this movie. I don't know if you know this movie. I don't know if you know this movie. I don't know if you know this movie. Sertin's Floor? It's kind of like popular from scientific perspective, but they got the problem, right? So this is the cyber, like not so cyberpunk, but probably like interesting science fiction. But take a look at the release year, it's 1999. The problem is that in 1999, this movie gets out, Matrix. And that's why everyone knows about Matrix. It was a very successful movie. And this is the reason much less people know that this movie, Sertin's Floor, even exists. If they released it like a year previous before Matrix, so much more success. The same with Whisper, right? So ChartGPT released November 14th and Whisper, actually the same company opened here, but it was not so hyped in September as it became in November, right? So what they introduced is they actually have been working like at the same time, looks like they've been working at the Whisper in parallel, like working with ChartGPT, right? So they reused the same, like not the same, but similar architecture, like encoders, decoders and tokens to predict the tokens, but not from text, but from audio as well. So they just take a look at the audio, right? And in the paper, so they share like this one. So they took 680,000 hours of multilingual audio. So they sliced it in 30 seconds, like steps, not steps, but chunks. And they trained the model, they trained the model against it using the same approach they use for the GPT. So pretty the same, like slice the data into some samples, try to predict the tokens, like compare with the result, learn, train the model and so on. And this results in a very interesting thing. Very high quality. And I don't know if there are any model right now which can beat the quality of Whisper. Because once it was released, it was like very high quality. They released another model a couple of months ago. I think it's called Whisper Turbo, but the architecture seems to stay the same. And one more interesting thing here is it's completely open sourced. So you can download it and it doesn't require a lot of VRAM to run. So you can run it on the GPU, you can run it a bit slower on the CPU. And as far as it's like not a large language model, it will perform like with a lot of other models. It will perform like with acceptable speed. I've been running it at the CPU once and it works normally, so not so slow as the LLM. So we are not so interested in technical details and I don't understand them actually just to explain to you. But what we're interested in is actually we're interested in the result, right? So how it works and how I can use it. So how to use it. So you just install it as a Python package, but you need PyTorch. And this is like why I mentioned the PyTorch previously. So in order to install it locally, you must have the PyTorch. And this is the... Come on, it should be PyTorch. Download probably? No. Come on, where's download? Get started. Oh, here, install. Okay, like if you would like to try Whisper, right? Or in future if you would like to work with PyTorch. So there's like two options here. So it doesn't matter which version you will be installing. You're going to see the same table for every time. Like it's a couple of years the same. So if you're installing it like in straightforward, like recommended way, like usually it's Windows, Python Packager, PIP, Python. And here is like CUDA or CPU. And depending on what you will select, the URL will be different, right? So CUDA is the accelerated computing framework created by Nvidia, as far as I remember by Nvidia, to run the computations on the GPU. And if you download and install this PyTorch, you will be running everything against the GPU. But if you do not have GPU, right, so you can install it without like just the CPU. And it will be the same PyTorch. It will work the same, but much more slower, right? But it will still work. So you need to prior to install the PyTorch. So first you need to Python. Next you need to select the CUDA version. So usually the later the better. You just copy this and install. And actually I did that today because my PyTorch, not PyTorch, but Whisper failed to run today. So I've been preparing for the demo and I also have like the some files, audio files. So and the problem is, so why facing them? And you may face the same. So let me show you. So the problem with Python is that somehow, like for modern applications, I use NumPy version 2. So NumPy is a framework, is a library to work in like with numbers in Python and it's required by PyTorch. But this PyTorch and its like implementation in Whisper doesn't work with NumPy 2.0.2. So it requires like old version. And that's this kind of problem you may face as well. So what I did is actually I installed like virtual environments. So I created the virtual Python environment and downloaded once again PyTorch, Whisper and all the stuff. So at least now it works, right? So that's the problem you may face with working with any software in Python. At least I think it should be some kind of work around, but taking a look at how many people using the virtual environments, I think this is the acceptable work around. So getting back here. So what you need is just to install the library. And that's it. You can install it from source, but I prefer like this lazy option. And it also requires FFmpeg. So FFmpeg, this is the most popular library to working with audio and video. So and actually this may be useful for you in future. So I know like working in development or just processing media like audio and video requires you to convert files from one format to another, split them, I don't know, change the bitrate and anything. So everything can be done with FFmpeg. But FFmpeg is a command line interface. Like it's a program which allows you to do everything, but without the user interface, without the UI. But you can work with it programmatically. So for example, this is how you can convert from MP4 to AV, like the old format. And this is like the greatest software and the fastest solution right now. I think so a lot of audio and video converting software use FFmpeg under the hood. So it will be necessary as well if you would like to start with Whisper because Whisper will need to transform the data from one format to another. So for example, in my examples, I'm going to use MP3, but in some cases it may be VAR format or something like that. So Whisper need to transform from one format to another. And here are the models and languages. So as far as it's a machine learning model, right, so you can pick up any model and getting back here. So remember, we've been discussing this like model card, quantization, distillation, and so on. So different models, right? So this is like 13 billion model. This is 70 billion model. The same with Whisper. So this model has 39 million parameters. It's tiny and it requires only one gigabyte of video RAM, like VRAM. And it's very fast, right? So assuming the large model has like 1.5 billion parameters, it will occupy 10 gigabytes of VRAM. And the speed is 1, like 1x, right? So this one is 10 times faster and Turbo is 8 times faster. And that's it. They have large V2 and large V3. And what's more interesting here is the quality. So the quality here is measured in WE error, like word error rate. So it's just evaluating how many words are mispredicted, like how many words are wrong. And you can see the languages here. So depending on the data set and probably the language structure, this really differs, right? So I'm not surprised the Spanish has less errors than any other languages because Spanish, at least from my perspective, I know a couple of words and it looks like very easy language, right? Like straightforward and kind of easy. Yeah, and some of the languages probably lacking the presence in the Internet will result in like half of the words misspelled, like Belorussian. I don't know. I think I need to... I know Belorussian language and I think I need to find some podcast in Belorussian and try to feed it into the whisper and check like how it works. But this is somewhat strange because 42 out of 100 comparing to 6 in Ukrainian, so Belorussian and Ukrainian are super similar. So I can understand like 90% of Ukrainian and Ukrainians understand 90% of Belorussian. Probably is the problem in the data set, right? Okay, but in most cases we're going to work with English, right? So working with these languages are super cool. And command line usage is pretty easy. So let's run the example. And for the example, I have the... I took the Matrix movie and you know, like there is a scene... Agent Smith is talking with Morpheus. Let me just... Matrix, Agent Smith. He's delivering the speech about like the humans. I'm not sure you will be able to hear it because I'm using Teams in browsers. So it's not desktop application, so I can't share my audio. But here is like the agent Smith is interrogating Morpheus and he's delivering the speech like I got the idea. So human beings are a virus and we, like the agents are the humans. So I will drop you the link or you can just Google it and reconsider. I picked this sample because it's like high quality speech and no any like music in the background and something like that. So let me try to run this and check how it will work. So default whisper. We just need to call whisper and pass the parameter as the data file. So right now is detecting the language using first 30 seconds and the language is detected to English. So you can do that actually manually. You can specify the language and here is the output, right? So this is like the Smith is discussing it. This is very precise. I think it's like probably here we have the problems. Yeah, I think it's missing. It's missing something still, but I don't know what model I'm use. So let me call the usage. OK, so a lot of things here. So we are interested in the model so we can specify the model from here, right? So these are the available models. I don't know which one is the default. Maybe. So I have a lot of VRAM so I can actually launch large model. So let's try with another model and you can also specify the language explicitly. I don't think it will help us because it automatically recognize that this is English. One more thing, actually, very interesting thing you can do with whisper. So here should be a parameter which is called prompt. You find it. OK, model device output verbose. There should be a parameter which is called something like a prompt. No. Yeah, this one initial prompt. So the trick is, so by default it's none, but let's get back to the idea. So whisper works with tokens, right? And everything like in this architecture, you can you can pass the system message. So you can pass the system message and whisper. And I've been working once on English transcription task. So we decided to transcribe the English assessments in our company. So there is like an interviewer asking questions and the person replying. And one of the idea was to calculate the I don't know how it's called, like the words like when the person is thinking like parasite words. So it's not possible to do with just plain whisper. Hardly possible. But if you explicitly provide them in a prompt, it will recognize them. And also a lot of times in this dialogue, they were mentioned to a company name, E-POM. And there is no such word in whisper data set. So that's why it was looking for a similar. But if you provide it in initial prompt and it will like see the same token combination, it will just do not search for a particular word. And it will do not search for any other alternative. So this is very powerful thing, like very underrated. OK, so let's get back here and let's try the model. Let me try different models and we're going to see the performance like this speed. Let's try with tiny, the smallest model, and let's check how fast it will work. And let me try turbo. OK, pretty fast as well. I don't see a big difference in speed here, right? At least this sample, the sample is only one minute. The final token was, took the time. And let me try large. So the large should give us the highest quality. OK, yeah, you see the difference. So large and turbo versus the tiny. So the tiny, you can just understand what's going on in this discussion. So if you don't care about particular words, if you just would like, I don't know, to summarize it in future and make some decision, like, I don't know, sentiment analysis or something. So maybe tiny is OK, but still, that's a lot of questions as far as I see. But here the turbo and the large, so they work very effectively. All right, so and once I run every model, so it's getting downloaded. So I don't think I started small, so probably it will be downloading right now. No, no, it's already downloaded, I think. Yeah. So once I selected large first time, it started downloading all of this model, like one and a half gigabytes of model locally. So if you have a lot of files, I don't know, million of hours, so what you just need, you just need the PC. So you can use it in cloud, right? So let's check the pricing. Open the icon, API pricing. So you can do that with API. Let me scroll to the whisper. Fine tuning, real time, assistance. OK, images, audio models. OK, so what we're going to pay, we're going to pay 0.16 cents per minute. So let's let me calculate one hour. So 0.6 cents per minute. So it means one hour will cost us 36 cents. Yeah, so it means like 100 hours of transcription will result in $36. So there should be some point where it's cheaper to purchase hardware and run it on your hardware and save the hardware for yourself instead of sending to the open AI, right? Yeah, so in some cases it's more effective to do it on your own machine. I didn't like spend the night to create some Python scripts. And yeah, regarding Python, you can call it from Python as well. So it's Python library. So you can use it from command line, but you can also, here is the command line user. You can also import it, load the model and transcribe your own data just as passing as the reference to the file, right? And here are the examples how you can do that with Python. Still, it has one problem which probably been solved in some forks of the whisper, like whisper X and so on. The problem is it doesn't differentiate speakers. So you will not see the difference between speaker one and speaker two, right? So what I did in homework, I shared the file with you where there is a difference between speaker one and speaker two. So here is no any difference, but the output formats may be different, right? So let me call the help once again. Here, output format. You can see we have a lot of different output formats. TXT, VTT, SRT, TSV, JSON, all. Let's try to run this all, but probably all is a default, right? So let me check. Okay, yeah, all is a default. It means I already have all this information available. So let's check it out. If you like JSONs, you can run this with the format. Very good, right?"
